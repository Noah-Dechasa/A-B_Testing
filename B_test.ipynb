{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Control = pd.read_csv('/Users/noah/Desktop/Work on Resume/A:B Testing/archive (3)/control_group.csv', delimiter= \";\")\n",
    "test = pd.read_csv('/Users/noah/Desktop/Work on Resume/A:B Testing/archive (3)/test_group.csv', delimiter= \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the dataset (number of rows and columns)\n",
    "print(f\"Control Group Shape: {Control.shape}\")\n",
    "print(f\"Test Group Shape: {test.shape}\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Control Group Sample:\")\n",
    "print(Control.head())\n",
    "\n",
    "print(\"Test Group Sample:\")\n",
    "print(test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in both the control and test datasets\n",
    "print(\"Control Group Missing Values:\")\n",
    "print(Control.isnull().sum())\n",
    "\n",
    "print(\"Test Group Missing Values:\")\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Visualize missing values using missingno\n",
    "msno.matrix(Control)\n",
    "msno.matrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "Control.dropna(inplace=True)\n",
    "\n",
    "# Summary statistics for control group\n",
    "print(\"Control Group Summary:\")\n",
    "print(Control.describe())\n",
    "\n",
    "# Summary statistics for test group\n",
    "print(\"Test Group Summary:\")\n",
    "print(test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating  Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for all key metrics\n",
    "metrics = ['# of Impressions', '# of Website Clicks', '# of Searches', '# of View Content', '# of Add to Cart', '# of Purchase']\n",
    "\n",
    "# Display summary statistics for control and test groups\n",
    "print(\"Control Group Summary for Key Metrics:\")\n",
    "print(Control[metrics].describe())\n",
    "\n",
    "print(\"\\nTest Group Summary for Key Metrics:\")\n",
    "print(test[metrics].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison for each key metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(Control[metric], color='blue', label='Control Group', kde=True)\n",
    "    sns.histplot(test[metric], color='red', label='Test Group', kde=True)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric} - Control vs Test')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison for each key metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(data=[Control[metric], test[metric]])\n",
    "    plt.xticks([0, 2], ['Control Group', 'Test Group'])  # Set x-axis labels for the groups\n",
    "    plt.title(f'{metric} - Control vs Test')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-test for # of Impressions:\n",
      "T-statistic: 4.884544325740239, P-value: 8.774394114329264e-06\n",
      "Reject the null hypothesis: There is a significant difference in # of Impressions between the groups.\n",
      "\n",
      "T-test for # of Website Clicks:\n",
      "T-statistic: -1.576909404840952, P-value: 0.12035072366063823\n",
      "Fail to reject the null hypothesis: There is no significant difference in # of Website Clicks between the groups.\n",
      "\n",
      "T-test for # of Searches:\n",
      "T-statistic: -1.1373340684043094, P-value: 0.26015715752487034\n",
      "Fail to reject the null hypothesis: There is no significant difference in # of Searches between the groups.\n",
      "\n",
      "T-test for # of View Content:\n",
      "T-statistic: 0.47615455602474466, P-value: 0.6357843704297139\n",
      "Fail to reject the null hypothesis: There is no significant difference in # of View Content between the groups.\n",
      "\n",
      "T-test for # of Add to Cart:\n",
      "T-statistic: 4.24906420944249, P-value: 8.032960071149043e-05\n",
      "Reject the null hypothesis: There is a significant difference in # of Add to Cart between the groups.\n",
      "\n",
      "T-test for # of Purchase:\n",
      "T-statistic: 0.03014479856562245, P-value: 0.9760568756579724\n",
      "Fail to reject the null hypothesis: There is no significant difference in # of Purchase between the groups.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform t-tests for each key metric\n",
    "for metric in metrics:\n",
    "    t_stat, p_value = stats.ttest_ind(Control[metric], test[metric])\n",
    "    print(f\"T-test for {metric}:\")\n",
    "    print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "    \n",
    "    # Interpret the result\n",
    "    if p_value < 0.05:\n",
    "        print(f\"Reject the null hypothesis: There is a significant difference in {metric} between the groups.\\n\")\n",
    "    else:\n",
    "        print(f\"Fail to reject the null hypothesis: There is no significant difference in {metric} between the groups.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's d for # of Impressions: -1.299935454973749\n",
      "Cohen's d for # of Website Clicks: 0.4105904871209517\n",
      "Cohen's d for # of Searches: 0.31503240638487723\n",
      "Cohen's d for # of View Content: -0.12477180753825634\n",
      "Cohen's d for # of Add to Cart: -1.1084589843445156\n",
      "Cohen's d for # of Purchase: -0.007876107565613272\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cohen's d for effect size for each significant metric\n",
    "for metric in metrics:\n",
    "    mean_control = Control[metric].mean()\n",
    "    mean_test = test[metric].mean()\n",
    "    std_control = Control[metric].std()\n",
    "    std_test = test[metric].std()\n",
    "    \n",
    "    cohens_d = (mean_test - mean_control) / ((std_control + std_test) / 2)\n",
    "    print(f\"Cohen's d for {metric}: {cohens_d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Results Summary\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "### 1. **# of Impressions**\n",
    "- **T-statistic**: 4.88\n",
    "- **P-value**: 8.77e-06\n",
    "- **Conclusion**: I found a **significant difference** between the control and test groups. This suggests that the changes I made in the test design had a noticeable impact on the number of impressions.\n",
    "\n",
    "### 2. **# of Website Clicks**\n",
    "- **T-statistic**: -1.58\n",
    "- **P-value**: 0.12\n",
    "- **Conclusion**: There was **no significant difference** in website clicks. The test group did not outperform the control group, indicating that the changes didn’t lead to a meaningful increase in clicks.\n",
    "\n",
    "### 3. **# of Searches**\n",
    "- **T-statistic**: -1.14\n",
    "- **P-value**: 0.26\n",
    "- **Conclusion**: There was **no significant difference** in the number of searches. The test group didn’t significantly affect how many users performed searches on the website.\n",
    "\n",
    "### 4. **# of View Content**\n",
    "- **T-statistic**: 0.48\n",
    "- **P-value**: 0.64\n",
    "- **Conclusion**: I found **no significant difference** in the number of content views. The test group did not lead to a substantial change in content interaction.\n",
    "\n",
    "### 5. **# of Add to Cart**\n",
    "- **T-statistic**: 4.25\n",
    "- **P-value**: 8.03e-05\n",
    "- **Conclusion**: There was a **significant difference** in the number of items added to the cart. This suggests that the test design had a positive impact on user engagement in terms of cart additions.\n",
    "\n",
    "### 6. **# of Purchases**\n",
    "- **T-statistic**: 0.03\n",
    "- **P-value**: 0.98\n",
    "- **Conclusion**: There was **no significant difference** in the number of purchases. The changes made to the test group did not result in a higher conversion rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Effect Size (Cohen’s d)\n",
    "\n",
    "### 1. **# of Impressions**\n",
    "- **Cohen’s d**: -1.30 (Large effect)\n",
    "  - Even though the control group had higher impressions, the difference is practically meaningful, indicating that the changes in the test group were impactful.\n",
    "\n",
    "### 2. **# of Website Clicks**\n",
    "- **Cohen’s d**: 0.41 (Medium effect)\n",
    "  - There was a moderate difference in website clicks between the two groups, but it wasn’t large enough to reach statistical significance.\n",
    "\n",
    "### 3. **# of Searches**\n",
    "- **Cohen’s d**: 0.32 (Small to medium effect)\n",
    "  - A small practical effect, suggesting the test didn’t influence searches in any major way.\n",
    "\n",
    "### 4. **# of View Content**\n",
    "- **Cohen’s d**: -0.12 (Very small effect)\n",
    "  - The difference is negligible, indicating that the changes did not significantly affect the number of content views.\n",
    "\n",
    "### 5. **# of Add to Cart**\n",
    "- **Cohen’s d**: -1.11 (Large effect)\n",
    "  - A substantial effect, indicating that the test design had a strong influence on the number of items users added to their cart.\n",
    "\n",
    "### 6. **# of Purchases**\n",
    "- **Cohen’s d**: -0.01 (Very small effect)\n",
    "  - There was no meaningful practical difference in purchases, confirming that the changes did not lead to more conversions.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Significant Differences**:\n",
    "   - I saw clear improvements in **Impressions** and **Add to Cart** for the test group, suggesting that the changes made had a noticeable impact in these areas. These could be valuable metrics to explore further.\n",
    "\n",
    "2. **No Significant Differences**:\n",
    "   - **Website Clicks**, **Searches**, **View Content**, and **Purchases** didn’t show significant differences, meaning the test group didn’t affect these behaviors in a meaningful way.\n",
    "\n",
    "3. **Effect Size**:\n",
    "   - Even though some metrics didn’t reach statistical significance (e.g., **Website Clicks**, **Purchases**), the **effect size** suggests that some of the differences could still be practically meaningful, especially in **Add to Cart**.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **For Improving Impressions and Add to Cart**:\n",
    "   - Since I saw significant differences in **Impressions** and **Add to Cart**, it would make sense to scale these changes. These metrics indicate that the test group had a positive impact, which could lead to better performance.\n",
    "\n",
    "2. **For Website Clicks and Purchases**:\n",
    "   - Given that **Website Clicks** and **Purchases** didn’t show significant improvements, I could consider testing other changes in design, UI, or features to improve conversion rates.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
